{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"learningOrchestra Docs learningOrchestra is a distributed processing tool that facilitates and streamlines iterative processes in a Data Science project pipeline like: Data Gathering Data Cleaning Model Building Validating the Model Presenting the Results With learningOrchestra, you can: load a dataset from an URL (in CSV format). accomplish several pre-processing tasks with datasets. create highly customised model predictions against a specific dataset by providing their own pre-processing code. build prediction models with different classifiers simultaneously using a spark cluster transparently. And so much more! Check the Usage section for more.","title":"Home"},{"location":"#learningorchestra-docs","text":"learningOrchestra is a distributed processing tool that facilitates and streamlines iterative processes in a Data Science project pipeline like: Data Gathering Data Cleaning Model Building Validating the Model Presenting the Results With learningOrchestra, you can: load a dataset from an URL (in CSV format). accomplish several pre-processing tasks with datasets. create highly customised model predictions against a specific dataset by providing their own pre-processing code. build prediction models with different classifiers simultaneously using a spark cluster transparently. And so much more! Check the Usage section for more.","title":"learningOrchestra Docs"},{"location":"database-api/","text":"/api/learningOrchestra/v1/dataset The dataset resource is used to manages datasets, datasets are downloaded in CSV format and parsed into JSON format where the primary key for each document is the datasetName field contained in the JSON file. Downloads dataset from URL POST CLUSTER_IP/api/learningOrchestra/v1/dataset Insert a CSV into the database using the POST method, JSON must be contained in the body of the HTTP request. The following fields are required: { \"datasetName\": \"key to dataset identification\", \"datasetURI\": \"http://sitetojson.file/path/to/csv\" } List dataset content GET CLUSTER_IP/api/learningOrchestra/v1/dataset/<datasetName>?skip=number&limit=number&query={} Returns rows of the dataset requested, with pagination. datasetName - Name of file requests skip - Amount of lines to skip in the CSV file limit - Limit the query result, maximum limit set to 20 rows query - Query to find documents, if only pagination is requested, query should be empty curly brackets query={} The first row in the query is always the metadata file. List all datasets metadata GET CLUSTER_IP/api/learningOrchestra/v1/dataset Returns an array of datasets metadata, where each dataset contains a metadata file. Datasets metadata { \"fields\": [ \"PassengerId\", \"Survived\", \"Pclass\", \"Name\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Ticket\", \"Fare\", \"Cabin\", \"Embarked\" ], \"datasetName\": \"titanicTraining\", \"finished\": true, \"type\": \"dataset\", \"timeCreated\": \"2020-07-28T22:16:10-00:00\", \"url\": \"https://filebin.net/rpfdy8clm5984a4c/titanic_training.csv?t=gcnjz1yo\" } fields - Names of the columns in the file datasetName - Name of the file finished - Flag used to indicate if asynchronous processing from file downloader is finished type - The type of file timeCreated - Time of creation url - URL used to download the file Delete a dataset DELETE CLUSTER_IP/api/learningOrchestra/v1/dataset/<datasetName> Request of type DELETE , passing the datasetName field of an existing dataset in the request parameters, deleting the dataset in the database.","title":"Dataset"},{"location":"database-api/#apilearningorchestrav1dataset","text":"The dataset resource is used to manages datasets, datasets are downloaded in CSV format and parsed into JSON format where the primary key for each document is the datasetName field contained in the JSON file.","title":"/api/learningOrchestra/v1/dataset"},{"location":"database-api/#downloads-dataset-from-url","text":"POST CLUSTER_IP/api/learningOrchestra/v1/dataset Insert a CSV into the database using the POST method, JSON must be contained in the body of the HTTP request. The following fields are required: { \"datasetName\": \"key to dataset identification\", \"datasetURI\": \"http://sitetojson.file/path/to/csv\" }","title":"Downloads dataset from URL"},{"location":"database-api/#list-dataset-content","text":"GET CLUSTER_IP/api/learningOrchestra/v1/dataset/<datasetName>?skip=number&limit=number&query={} Returns rows of the dataset requested, with pagination. datasetName - Name of file requests skip - Amount of lines to skip in the CSV file limit - Limit the query result, maximum limit set to 20 rows query - Query to find documents, if only pagination is requested, query should be empty curly brackets query={} The first row in the query is always the metadata file.","title":"List dataset content"},{"location":"database-api/#list-all-datasets-metadata","text":"GET CLUSTER_IP/api/learningOrchestra/v1/dataset Returns an array of datasets metadata, where each dataset contains a metadata file.","title":"List all datasets metadata"},{"location":"database-api/#datasets-metadata","text":"{ \"fields\": [ \"PassengerId\", \"Survived\", \"Pclass\", \"Name\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Ticket\", \"Fare\", \"Cabin\", \"Embarked\" ], \"datasetName\": \"titanicTraining\", \"finished\": true, \"type\": \"dataset\", \"timeCreated\": \"2020-07-28T22:16:10-00:00\", \"url\": \"https://filebin.net/rpfdy8clm5984a4c/titanic_training.csv?t=gcnjz1yo\" } fields - Names of the columns in the file datasetName - Name of the file finished - Flag used to indicate if asynchronous processing from file downloader is finished type - The type of file timeCreated - Time of creation url - URL used to download the file","title":"Datasets metadata"},{"location":"database-api/#delete-a-dataset","text":"DELETE CLUSTER_IP/api/learningOrchestra/v1/dataset/<datasetName> Request of type DELETE , passing the datasetName field of an existing dataset in the request parameters, deleting the dataset in the database.","title":"Delete a dataset"},{"location":"datatype-api/","text":"/api/learningOrchestra/v1/transform/dataType The dataType resource changes the fields data type from stored datasets between number and string . Change field types of a dataset PATCH CLUSTER_IP/api/learningOrchestra/v1/transform/dataType The request uses inputDatasetName as the dataset name to change the field types, and the types uses a number or string descriptor in each Key:Value to describe the new value of altered field in dataset. { \"inputDatasetName\": \"SomeDatasetName\", \"types\": { \"field1\": \"number\", \"field2\": \"string\" } }","title":"Data Type"},{"location":"datatype-api/#apilearningorchestrav1transformdatatype","text":"The dataType resource changes the fields data type from stored datasets between number and string .","title":"/api/learningOrchestra/v1/transform/dataType"},{"location":"datatype-api/#change-field-types-of-a-dataset","text":"PATCH CLUSTER_IP/api/learningOrchestra/v1/transform/dataType The request uses inputDatasetName as the dataset name to change the field types, and the types uses a number or string descriptor in each Key:Value to describe the new value of altered field in dataset. { \"inputDatasetName\": \"SomeDatasetName\", \"types\": { \"field1\": \"number\", \"field2\": \"string\" } }","title":"Change field types of a dataset"},{"location":"histogram-api/","text":"/api/learningOrchestra/v1/explore/histogram The histogram resource makes a histogram from a stored file, storing the resulting histogram in a new dataset. Create a histogram from a dataset POST CLUSTER_IP/api/learningOrchestra/v1/explore/histogram The request is sent in the body, inputDatasetName is the name from used dataset and the outputDatasetName is the name of the file in which the histogram result is saved. The names is an array with all the fields necessary to make the histogram. { \"inputDatasetName\": \"dataset input name to use\", \"outputDatasetName\": \"name to histogram dataset\", \"names\" : [\"list\", \"of\", \"fields\"] } List histogram dataset content GET CLUSTER_IP/api/learningOrchestra/v1/explore/histogram/<datasetName>?skip=number&limit=number&query={} Returns rows of the dataset requested, with pagination. datasetName - Name of file requests skip - Amount of lines to skip in the CSV file limit - Limit the query result, maximum limit set to 20 rows query - Query to find documents, if only pagination is requested, query should be empty curly brackets query={} The first row in the query is always the metadata file. List all histogram datasets metadata GET CLUSTER_IP/api/learningOrchestra/v1/explore/histogram Returns an array of histogram datasets metadata, where each dataset contains a metadata file. Histogram datasets metadata { \"fields\": [ \"PassengerId\", \"Survived\", \"Pclass\", \"Name\", \"Sex\" ], \"datasetName\": \"titanicTrainingHistogram\", \"parentDatasetName\": \"titanicTraining\", \"finished\": true, \"type\": \"histogram\", \"timeCreated\": \"2020-07-28T22:16:10-00:00\", \"url\": \"https://filebin.net/rpfdy8clm5984a4c/titanic_training.csv?t=gcnjz1yo\" } fields - Names of the columns in the file parentDatasetName - The datasetName used to make the histogram dataset, from which the current dataset is derived. datasetName - Name of the file finished - Flag used to indicate if asynchronous processing from file downloader is finished type - The type of file timeCreated - Time of creation url - URL used to download the file Delete a histogram dataset DELETE CLUSTER_IP/api/learningOrchestra/v1/explore/histogram/<datasetName> Request of type DELETE , passing the datasetName field of an histogram dataset in the request parameters, deleting the dataset.","title":"Histogram"},{"location":"histogram-api/#apilearningorchestrav1explorehistogram","text":"The histogram resource makes a histogram from a stored file, storing the resulting histogram in a new dataset.","title":"/api/learningOrchestra/v1/explore/histogram"},{"location":"histogram-api/#create-a-histogram-from-a-dataset","text":"POST CLUSTER_IP/api/learningOrchestra/v1/explore/histogram The request is sent in the body, inputDatasetName is the name from used dataset and the outputDatasetName is the name of the file in which the histogram result is saved. The names is an array with all the fields necessary to make the histogram. { \"inputDatasetName\": \"dataset input name to use\", \"outputDatasetName\": \"name to histogram dataset\", \"names\" : [\"list\", \"of\", \"fields\"] }","title":"Create a histogram from a dataset"},{"location":"histogram-api/#list-histogram-dataset-content","text":"GET CLUSTER_IP/api/learningOrchestra/v1/explore/histogram/<datasetName>?skip=number&limit=number&query={} Returns rows of the dataset requested, with pagination. datasetName - Name of file requests skip - Amount of lines to skip in the CSV file limit - Limit the query result, maximum limit set to 20 rows query - Query to find documents, if only pagination is requested, query should be empty curly brackets query={} The first row in the query is always the metadata file.","title":"List histogram dataset content"},{"location":"histogram-api/#list-all-histogram-datasets-metadata","text":"GET CLUSTER_IP/api/learningOrchestra/v1/explore/histogram Returns an array of histogram datasets metadata, where each dataset contains a metadata file.","title":"List all histogram datasets metadata"},{"location":"histogram-api/#histogram-datasets-metadata","text":"{ \"fields\": [ \"PassengerId\", \"Survived\", \"Pclass\", \"Name\", \"Sex\" ], \"datasetName\": \"titanicTrainingHistogram\", \"parentDatasetName\": \"titanicTraining\", \"finished\": true, \"type\": \"histogram\", \"timeCreated\": \"2020-07-28T22:16:10-00:00\", \"url\": \"https://filebin.net/rpfdy8clm5984a4c/titanic_training.csv?t=gcnjz1yo\" } fields - Names of the columns in the file parentDatasetName - The datasetName used to make the histogram dataset, from which the current dataset is derived. datasetName - Name of the file finished - Flag used to indicate if asynchronous processing from file downloader is finished type - The type of file timeCreated - Time of creation url - URL used to download the file","title":"Histogram datasets metadata"},{"location":"histogram-api/#delete-a-histogram-dataset","text":"DELETE CLUSTER_IP/api/learningOrchestra/v1/explore/histogram/<datasetName> Request of type DELETE , passing the datasetName field of an histogram dataset in the request parameters, deleting the dataset.","title":"Delete a histogram dataset"},{"location":"installation/","text":"Installation Requirements Debian Linux hosts 120 GB of disk in each cluster machine (can increase relying on dataset size) 8 GB of RAM in each cluster machine (can increase relying on dataset size) Docker Engine must be installed in all instances of your cluster Cluster configured in swarm mode, check creating a swarm Docker Compose must be installed in the manager instance of your cluster Ensure that your cluster environment does not block any traffic such as firewall rules in your network or in your hosts. If in case, you have firewalls or other traffic-blockers, add learningOrchestra as an exception. Ex: In Google Cloud Platform each of the VMs must allow both http and https traffic. Deployment In the manager Docker swarm machine, clone the repo using: git clone https://github.com/riibeirogabriel/learningOrchestra.git Navigate into the learningOrchestra directory and run: cd learningOrchestra sudo ./run.sh That's it! learningOrchestra has been deployed in your swarm cluster! Cluster State CLUSTER_IP:8000 - To visualize cluster state (deployed microservices and cluster's machines). CLUSTER_IP:8080 - To visualize spark cluster state. CLUSTER_IP is the external IP of a machine in your cluster.","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#requirements","text":"Debian Linux hosts 120 GB of disk in each cluster machine (can increase relying on dataset size) 8 GB of RAM in each cluster machine (can increase relying on dataset size) Docker Engine must be installed in all instances of your cluster Cluster configured in swarm mode, check creating a swarm Docker Compose must be installed in the manager instance of your cluster Ensure that your cluster environment does not block any traffic such as firewall rules in your network or in your hosts. If in case, you have firewalls or other traffic-blockers, add learningOrchestra as an exception. Ex: In Google Cloud Platform each of the VMs must allow both http and https traffic.","title":"Requirements"},{"location":"installation/#deployment","text":"In the manager Docker swarm machine, clone the repo using: git clone https://github.com/riibeirogabriel/learningOrchestra.git Navigate into the learningOrchestra directory and run: cd learningOrchestra sudo ./run.sh That's it! learningOrchestra has been deployed in your swarm cluster!","title":"Deployment"},{"location":"installation/#cluster-state","text":"CLUSTER_IP:8000 - To visualize cluster state (deployed microservices and cluster's machines). CLUSTER_IP:8080 - To visualize spark cluster state. CLUSTER_IP is the external IP of a machine in your cluster.","title":"Cluster State"},{"location":"modelbuilder-api/","text":"/api/learningOrchestra/v1/builder The builder resource join several steps of machine learning workflow (transform, tune, train and evaluate) acoupling in a unique resouce, builder creates several model predictions using your own modeling code using a defined set of classifiers. Create a builder POST CLUSTER_IP/api/learningOrchestra/v1/builder { \"trainDatasetName\": \"trainDataset\", \"testDatasetName\": \"testDataset\", \"modelingCode\": \"Python3 code to preprocessing, using Pyspark library\", \"classifiersList\": \"String list of classifiers to be used\" } List of Classifiers LR : LogisticRegression DT : DecisionTreeClassifier RF : RandomForestClassifier GB : Gradient-boosted tree classifier NB : NaiveBayes To send a request with LogisticRegression and NaiveBayes Classifiers: { \"trainDatasetName\": \"trainDataset\", \"testDatasetName\": \"testDataset\", \"modeling_code\": \"Python3 code to preprocessing, using Pyspark library\", \"classifiers_list\": [\"LR\", \"NB\"] } modeling_code environment The python 3 modeling code must use the environment instances in bellow: training_df (Instantiated): Spark Dataframe instance of train dataset testing_df (Instantiated): Spark Dataframe instance of testing dataset The modeling code must instantiate the variables in below, all instances must be transformed by pyspark VectorAssembler: features_training (Not Instantiated): Spark Dataframe instance for train the model features_evaluation (Not Instantiated): Spark Dataframe instance for evaluating trained model accuracy features_testing (Not Instantiated): Spark Dataframe instance for testing the model In case you don't want to evaluate the model, set features_evaluation as None . Handy methods self.fields_from_dataframe(self, dataframe, is_string) This method returns string or number fields as a string list from a DataFrame. dataframe : DataFrame instance is_string : Boolean parameter, if True , the method returns the string DataFrame fields, otherwise, returns the numbers DataFrame fields. modeling_code Example This example uses the titanic challengue datasets . from pyspark.ml import Pipeline from pyspark.sql.functions import ( mean, col, split, regexp_extract, when, lit) from pyspark.ml.feature import ( VectorAssembler, StringIndexer ) TRAINING_DF_INDEX = 0 TESTING_DF_INDEX = 1 training_df = training_df.withColumnRenamed('Survived', 'label') testing_df = testing_df.withColumn('label', lit(0)) datasets_list = [training_df, testing_df] for index, dataset in enumerate(datasets_list): dataset = dataset.withColumn( \"Initial\", regexp_extract(col(\"Name\"), \"([A-Za-z]+)\\.\", 1)) datasets_list[index] = dataset misspelled_initials = [ 'Mlle', 'Mme', 'Ms', 'Dr', 'Major', 'Lady', 'Countess', 'Jonkheer', 'Col', 'Rev', 'Capt', 'Sir', 'Don' ] correct_initials = [ 'Miss', 'Miss', 'Miss', 'Mr', 'Mr', 'Mrs', 'Mrs', 'Other', 'Other', 'Other', 'Mr', 'Mr', 'Mr' ] for index, dataset in enumerate(datasets_list): dataset = dataset.replace(misspelled_initials, correct_initials) datasets_list[index] = dataset initials_age = {\"Miss\": 22, \"Other\": 46, \"Master\": 5, \"Mr\": 33, \"Mrs\": 36} for index, dataset in enumerate(datasets_list): for initial, initial_age in initials_age.items(): dataset = dataset.withColumn( \"Age\", when((dataset[\"Initial\"] == initial) & (dataset[\"Age\"].isNull()), initial_age).otherwise( dataset[\"Age\"])) datasets_list[index] = dataset for index, dataset in enumerate(datasets_list): dataset = dataset.na.fill({\"Embarked\": 'S'}) datasets_list[index] = dataset for index, dataset in enumerate(datasets_list): dataset = dataset.withColumn(\"Family_Size\", col('SibSp')+col('Parch')) dataset = dataset.withColumn('Alone', lit(0)) dataset = dataset.withColumn( \"Alone\", when(dataset[\"Family_Size\"] == 0, 1).otherwise(dataset[\"Alone\"])) datasets_list[index] = dataset text_fields = [\"Sex\", \"Embarked\", \"Initial\"] for column in text_fields: for index, dataset in enumerate(datasets_list): dataset = StringIndexer( inputCol=column, outputCol=column+\"_index\").\\ fit(dataset).\\ transform(dataset) datasets_list[index] = dataset non_required_columns = [\"Name\", \"Embarked\", \"Sex\", \"Initial\"] for index, dataset in enumerate(datasets_list): dataset = dataset.drop(*non_required_columns) datasets_list[index] = dataset training_df = datasets_list[TRAINING_DF_INDEX] testing_df = datasets_list[TESTING_DF_INDEX] assembler = VectorAssembler( inputCols=training_df.columns[:], outputCol=\"features\") assembler.setHandleInvalid('skip') features_training = assembler.transform(training_df) (features_training, features_evaluation) =\\ features_training.randomSplit([0.8, 0.2], seed=33) features_testing = assembler.transform(testing_df) List builder dataset content GET CLUSTER_IP/api/learningOrchestra/v1/builder/<datasetName>?skip=number&limit=number&query={} Returns rows of the dataset requested, with pagination. datasetName - Name of file requests skip - Amount of lines to skip in the CSV file limit - Limit the query result, maximum limit set to 20 rows query - Query to find documents, if only pagination is requested, query should be empty curly brackets query={} The first row in the query is always the metadata file. List all builder datasets metadata GET CLUSTER_IP/api/learningOrchestra/v1/builder Returns an array of builder datasets metadata, where each dataset contains a metadata file. Builder datasets metadata { \"classifier\": \"NB\", \"datasetName\": \"titanicTestNB\", \"finished\": true, \"parentDatasetName\": [\"titanicTrain\", \"titanicTest\"], \"timeCreated\": \"2020-11-04T10:51:43-00:00\", \"type\": \"builder\" } classifier - The classifier used to make the predictions parentDatasetName - The train and test dataset names used to make the builder dataset, from which the current dataset is derived. datasetName - Name of the file finished - Flag used to indicate if asynchronous processing from file downloader is finished type - The type of file timeCreated - Time of creation Delete a builder dataset DELETE CLUSTER_IP/api/learningOrchestra/v1/builder/<datasetName> Request of type DELETE , passing the datasetName field of a builder dataset in the request parameters, deleting the dataset.","title":"Builder"},{"location":"modelbuilder-api/#apilearningorchestrav1builder","text":"The builder resource join several steps of machine learning workflow (transform, tune, train and evaluate) acoupling in a unique resouce, builder creates several model predictions using your own modeling code using a defined set of classifiers.","title":"/api/learningOrchestra/v1/builder"},{"location":"modelbuilder-api/#create-a-builder","text":"POST CLUSTER_IP/api/learningOrchestra/v1/builder { \"trainDatasetName\": \"trainDataset\", \"testDatasetName\": \"testDataset\", \"modelingCode\": \"Python3 code to preprocessing, using Pyspark library\", \"classifiersList\": \"String list of classifiers to be used\" }","title":"Create a builder"},{"location":"modelbuilder-api/#list-of-classifiers","text":"LR : LogisticRegression DT : DecisionTreeClassifier RF : RandomForestClassifier GB : Gradient-boosted tree classifier NB : NaiveBayes To send a request with LogisticRegression and NaiveBayes Classifiers: { \"trainDatasetName\": \"trainDataset\", \"testDatasetName\": \"testDataset\", \"modeling_code\": \"Python3 code to preprocessing, using Pyspark library\", \"classifiers_list\": [\"LR\", \"NB\"] }","title":"List of Classifiers"},{"location":"modelbuilder-api/#modeling_code-environment","text":"The python 3 modeling code must use the environment instances in bellow: training_df (Instantiated): Spark Dataframe instance of train dataset testing_df (Instantiated): Spark Dataframe instance of testing dataset The modeling code must instantiate the variables in below, all instances must be transformed by pyspark VectorAssembler: features_training (Not Instantiated): Spark Dataframe instance for train the model features_evaluation (Not Instantiated): Spark Dataframe instance for evaluating trained model accuracy features_testing (Not Instantiated): Spark Dataframe instance for testing the model In case you don't want to evaluate the model, set features_evaluation as None .","title":"modeling_code environment"},{"location":"modelbuilder-api/#handy-methods","text":"self.fields_from_dataframe(self, dataframe, is_string) This method returns string or number fields as a string list from a DataFrame. dataframe : DataFrame instance is_string : Boolean parameter, if True , the method returns the string DataFrame fields, otherwise, returns the numbers DataFrame fields.","title":"Handy methods"},{"location":"modelbuilder-api/#modeling_code-example","text":"This example uses the titanic challengue datasets . from pyspark.ml import Pipeline from pyspark.sql.functions import ( mean, col, split, regexp_extract, when, lit) from pyspark.ml.feature import ( VectorAssembler, StringIndexer ) TRAINING_DF_INDEX = 0 TESTING_DF_INDEX = 1 training_df = training_df.withColumnRenamed('Survived', 'label') testing_df = testing_df.withColumn('label', lit(0)) datasets_list = [training_df, testing_df] for index, dataset in enumerate(datasets_list): dataset = dataset.withColumn( \"Initial\", regexp_extract(col(\"Name\"), \"([A-Za-z]+)\\.\", 1)) datasets_list[index] = dataset misspelled_initials = [ 'Mlle', 'Mme', 'Ms', 'Dr', 'Major', 'Lady', 'Countess', 'Jonkheer', 'Col', 'Rev', 'Capt', 'Sir', 'Don' ] correct_initials = [ 'Miss', 'Miss', 'Miss', 'Mr', 'Mr', 'Mrs', 'Mrs', 'Other', 'Other', 'Other', 'Mr', 'Mr', 'Mr' ] for index, dataset in enumerate(datasets_list): dataset = dataset.replace(misspelled_initials, correct_initials) datasets_list[index] = dataset initials_age = {\"Miss\": 22, \"Other\": 46, \"Master\": 5, \"Mr\": 33, \"Mrs\": 36} for index, dataset in enumerate(datasets_list): for initial, initial_age in initials_age.items(): dataset = dataset.withColumn( \"Age\", when((dataset[\"Initial\"] == initial) & (dataset[\"Age\"].isNull()), initial_age).otherwise( dataset[\"Age\"])) datasets_list[index] = dataset for index, dataset in enumerate(datasets_list): dataset = dataset.na.fill({\"Embarked\": 'S'}) datasets_list[index] = dataset for index, dataset in enumerate(datasets_list): dataset = dataset.withColumn(\"Family_Size\", col('SibSp')+col('Parch')) dataset = dataset.withColumn('Alone', lit(0)) dataset = dataset.withColumn( \"Alone\", when(dataset[\"Family_Size\"] == 0, 1).otherwise(dataset[\"Alone\"])) datasets_list[index] = dataset text_fields = [\"Sex\", \"Embarked\", \"Initial\"] for column in text_fields: for index, dataset in enumerate(datasets_list): dataset = StringIndexer( inputCol=column, outputCol=column+\"_index\").\\ fit(dataset).\\ transform(dataset) datasets_list[index] = dataset non_required_columns = [\"Name\", \"Embarked\", \"Sex\", \"Initial\"] for index, dataset in enumerate(datasets_list): dataset = dataset.drop(*non_required_columns) datasets_list[index] = dataset training_df = datasets_list[TRAINING_DF_INDEX] testing_df = datasets_list[TESTING_DF_INDEX] assembler = VectorAssembler( inputCols=training_df.columns[:], outputCol=\"features\") assembler.setHandleInvalid('skip') features_training = assembler.transform(training_df) (features_training, features_evaluation) =\\ features_training.randomSplit([0.8, 0.2], seed=33) features_testing = assembler.transform(testing_df)","title":"modeling_code Example"},{"location":"modelbuilder-api/#list-builder-dataset-content","text":"GET CLUSTER_IP/api/learningOrchestra/v1/builder/<datasetName>?skip=number&limit=number&query={} Returns rows of the dataset requested, with pagination. datasetName - Name of file requests skip - Amount of lines to skip in the CSV file limit - Limit the query result, maximum limit set to 20 rows query - Query to find documents, if only pagination is requested, query should be empty curly brackets query={} The first row in the query is always the metadata file.","title":"List builder dataset content"},{"location":"modelbuilder-api/#list-all-builder-datasets-metadata","text":"GET CLUSTER_IP/api/learningOrchestra/v1/builder Returns an array of builder datasets metadata, where each dataset contains a metadata file.","title":"List all builder datasets metadata"},{"location":"modelbuilder-api/#builder-datasets-metadata","text":"{ \"classifier\": \"NB\", \"datasetName\": \"titanicTestNB\", \"finished\": true, \"parentDatasetName\": [\"titanicTrain\", \"titanicTest\"], \"timeCreated\": \"2020-11-04T10:51:43-00:00\", \"type\": \"builder\" } classifier - The classifier used to make the predictions parentDatasetName - The train and test dataset names used to make the builder dataset, from which the current dataset is derived. datasetName - Name of the file finished - Flag used to indicate if asynchronous processing from file downloader is finished type - The type of file timeCreated - Time of creation","title":"Builder datasets metadata"},{"location":"modelbuilder-api/#delete-a-builder-dataset","text":"DELETE CLUSTER_IP/api/learningOrchestra/v1/builder/<datasetName> Request of type DELETE , passing the datasetName field of a builder dataset in the request parameters, deleting the dataset.","title":"Delete a builder dataset"},{"location":"pca-api/","text":"/api/learningOrchestra/v1/explore/pca PCA is used to decompose a multivariate dataset in a set of successive orthogonal components that explain a maximum amount of the variance. In scikit-learn (used in this microservice), PCA is implemented as a transformer object that learns components in its fit method, and can be used on new data to project it on these components. This is often useful if the models down-stream make strong assumptions on the isotropy of the signal: this is for example the case for Support Vector Machines with the RBF kernel and the K-Means clustering algorithm, more information about this algorithm in scikit-learn PCA docs . The pca resource uses the PCA method on a dataset, and create a plot image from result. Create a PCA image plot POST CLUSTER_IP/api/learningOrchestra/v1/explore/pca The body contains the json fields: { \"inputDatasetName\": \"used dataset name\", \"outputPlotName\": \"result image plot name\", \"label\": \"dataset label column\" } The label is the label name column for machine learning datasets which has labeled tuples. In the case that the dataset used doesn't contain labeled tuples, define the value as null type in JSON: { \"inputDatasetName\": \"used dataset name\", \"outputPlotName\": \"result image plot name\", \"label\": null } Read the plot names of the created images GET CLUSTER_IP/api/learningOrchestra/v1/explore/pca Returns a list with all created image plot names. Read an image plot GET CLUSTER_IP/api/learningOrchestra/v1/explore/pca/<plotName> Returns the image plot of plotName specified. Delete an image plot DELETE CLUSTER_IP/api/learningOrchestra/v1/explore/pca/<plotName> Deletes an image plot from the database. Images plot examples This examples use the titanic challengue datasets . Titanic Train dataset Titanic Test dataset","title":"PCA"},{"location":"pca-api/#apilearningorchestrav1explorepca","text":"PCA is used to decompose a multivariate dataset in a set of successive orthogonal components that explain a maximum amount of the variance. In scikit-learn (used in this microservice), PCA is implemented as a transformer object that learns components in its fit method, and can be used on new data to project it on these components. This is often useful if the models down-stream make strong assumptions on the isotropy of the signal: this is for example the case for Support Vector Machines with the RBF kernel and the K-Means clustering algorithm, more information about this algorithm in scikit-learn PCA docs . The pca resource uses the PCA method on a dataset, and create a plot image from result.","title":"/api/learningOrchestra/v1/explore/pca"},{"location":"pca-api/#create-a-pca-image-plot","text":"POST CLUSTER_IP/api/learningOrchestra/v1/explore/pca The body contains the json fields: { \"inputDatasetName\": \"used dataset name\", \"outputPlotName\": \"result image plot name\", \"label\": \"dataset label column\" } The label is the label name column for machine learning datasets which has labeled tuples. In the case that the dataset used doesn't contain labeled tuples, define the value as null type in JSON: { \"inputDatasetName\": \"used dataset name\", \"outputPlotName\": \"result image plot name\", \"label\": null }","title":"Create a PCA image plot"},{"location":"pca-api/#read-the-plot-names-of-the-created-images","text":"GET CLUSTER_IP/api/learningOrchestra/v1/explore/pca Returns a list with all created image plot names.","title":"Read the plot names of the created images"},{"location":"pca-api/#read-an-image-plot","text":"GET CLUSTER_IP/api/learningOrchestra/v1/explore/pca/<plotName> Returns the image plot of plotName specified.","title":"Read an image plot"},{"location":"pca-api/#delete-an-image-plot","text":"DELETE CLUSTER_IP/api/learningOrchestra/v1/explore/pca/<plotName> Deletes an image plot from the database.","title":"Delete an image plot"},{"location":"pca-api/#images-plot-examples","text":"This examples use the titanic challengue datasets .","title":"Images plot examples"},{"location":"pca-api/#titanic-train-dataset","text":"","title":"Titanic Train dataset"},{"location":"pca-api/#titanic-test-dataset","text":"","title":"Titanic Test dataset"},{"location":"projection-api/","text":"/api/learningOrchestra/v1/transform/projection The projection resource makes a projection from dataset inserted in dataset resource, generating a new dataset, and also manages this generated dataset. Create a projection dataset from an inserted dataset POST CLUSTER_IP:/api/learningOrchestra/v1/transform/projection { \"inputDatasetName\": \"name of dataset to use\", \"outputDatasetName\": \"name of projection dataset\", \"names\" : [\"list\", \"of\", \"fields\"] } List projection dataset content GET CLUSTER_IP/api/learningOrchestra/v1/transform/projection/<datasetName>?skip=number&limit=number&query={} Returns rows of the dataset requested, with pagination. datasetName - Name of file requests skip - Amount of lines to skip in the CSV file limit - Limit the query result, maximum limit set to 20 rows query - Query to find documents, if only pagination is requested, query should be empty curly brackets query={} The first row in the query is always the metadata file. List all projection datasets metadata GET CLUSTER_IP/api/learningOrchestra/v1/transform/projection Returns an array of projection datasets metadata, where each dataset contains a metadata file. Projection datasets metadata { \"fields\": [ \"PassengerId\", \"Survived\", \"Pclass\", \"Name\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Ticket\", \"Fare\", \"Cabin\", \"Embarked\" ], \"datasetName\": \"titanicTrainingProjection\", \"parentDatasetName\": \"titanicTraining\", \"finished\": true, \"type\": \"projection\", \"timeCreated\": \"2020-07-28T22:16:10-00:00\", \"url\": \"https://filebin.net/rpfdy8clm5984a4c/titanic_training.csv?t=gcnjz1yo\" } fields - Names of the columns in the file parentFatasetName - The datasetName used to make the projection dataset, from which the current dataset is derived. datasetName - Name of the file finished - Flag used to indicate if asynchronous processing from file downloader is finished type - The type of file timeCreated - Time of creation url - URL used to download the file Delete a projection dataset DELETE CLUSTER_IP/api/learningOrchestra/v1/transform/projection/<datasetName> Request of type DELETE , passing the datasetName field of an projection dataset in the request parameters, deleting the dataset.","title":"Projection"},{"location":"projection-api/#apilearningorchestrav1transformprojection","text":"The projection resource makes a projection from dataset inserted in dataset resource, generating a new dataset, and also manages this generated dataset.","title":"/api/learningOrchestra/v1/transform/projection"},{"location":"projection-api/#create-a-projection-dataset-from-an-inserted-dataset","text":"POST CLUSTER_IP:/api/learningOrchestra/v1/transform/projection { \"inputDatasetName\": \"name of dataset to use\", \"outputDatasetName\": \"name of projection dataset\", \"names\" : [\"list\", \"of\", \"fields\"] }","title":"Create a projection dataset from an inserted dataset"},{"location":"projection-api/#list-projection-dataset-content","text":"GET CLUSTER_IP/api/learningOrchestra/v1/transform/projection/<datasetName>?skip=number&limit=number&query={} Returns rows of the dataset requested, with pagination. datasetName - Name of file requests skip - Amount of lines to skip in the CSV file limit - Limit the query result, maximum limit set to 20 rows query - Query to find documents, if only pagination is requested, query should be empty curly brackets query={} The first row in the query is always the metadata file.","title":"List projection dataset content"},{"location":"projection-api/#list-all-projection-datasets-metadata","text":"GET CLUSTER_IP/api/learningOrchestra/v1/transform/projection Returns an array of projection datasets metadata, where each dataset contains a metadata file.","title":"List all projection datasets metadata"},{"location":"projection-api/#projection-datasets-metadata","text":"{ \"fields\": [ \"PassengerId\", \"Survived\", \"Pclass\", \"Name\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Ticket\", \"Fare\", \"Cabin\", \"Embarked\" ], \"datasetName\": \"titanicTrainingProjection\", \"parentDatasetName\": \"titanicTraining\", \"finished\": true, \"type\": \"projection\", \"timeCreated\": \"2020-07-28T22:16:10-00:00\", \"url\": \"https://filebin.net/rpfdy8clm5984a4c/titanic_training.csv?t=gcnjz1yo\" } fields - Names of the columns in the file parentFatasetName - The datasetName used to make the projection dataset, from which the current dataset is derived. datasetName - Name of the file finished - Flag used to indicate if asynchronous processing from file downloader is finished type - The type of file timeCreated - Time of creation url - URL used to download the file","title":"Projection datasets metadata"},{"location":"projection-api/#delete-a-projection-dataset","text":"DELETE CLUSTER_IP/api/learningOrchestra/v1/transform/projection/<datasetName> Request of type DELETE , passing the datasetName field of an projection dataset in the request parameters, deleting the dataset.","title":"Delete a projection dataset"},{"location":"python-apis/","text":"Python Client The Python Client is a python package to use the learningOrchestra API, you can see how use this package on package repo .","title":"Python Client"},{"location":"python-apis/#python-client","text":"The Python Client is a python package to use the learningOrchestra API, you can see how use this package on package repo .","title":"Python Client"},{"location":"t-sne-api/","text":"/api/learningOrchestra/v1/explore/tsne The T-distributed Stochastic Neighbor Embedding (t-SNE) is a machine learning algorithm for visualization. It is a nonlinear dimensionality reduction technique well-suited for embedding high-dimensional data for visualization in a low-dimensional space of two or three dimensions. Specifically, it models each high-dimensional object by a two- or three-dimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points with high probability, more information about this algorithm in its Wiki page . The tsne resource uses the t-SNE method on a dataset, and create a plot image from result. Create an image plot POST CLUSTER_IP/api/learningOrchestra/v1/explore/tsne The body contains the json fields: { \"inputDatasetName\": \"used dataset name\", \"outputPlotName\": \"result image plot name\", \"label\": \"dataset label column\" } The label is the label name of the column for machine learning datasets which has labeled tuples. In the case that the dataset used doesn't contain labeled tuples, define the value as null type in json: { \"inputDatasetName\": \"used dataset name\", \"outputPlotName\": \"result image plot name\", \"label\": null } Read the names of the created plots GET CLUSTER_IP/api/learningOrchestra/v1/explore/tsne Returns a list with all created images plot names. Read an image plot GET CLUSTER_IP/api/learningOrchestra/v1/explore/tsne/<plotName> Returns the image plot of the specified plotName. Delete an image plot DELETE CLUSTER_IP/api/learningOrchestra/v1/explore/tsne/<plotName> Deletes an image plot by specifying its plotName. Image plot examples These examples use the titanic challengue datasets . Titanic Train dataset Titanic Test dataset","title":"t-SNE"},{"location":"t-sne-api/#apilearningorchestrav1exploretsne","text":"The T-distributed Stochastic Neighbor Embedding (t-SNE) is a machine learning algorithm for visualization. It is a nonlinear dimensionality reduction technique well-suited for embedding high-dimensional data for visualization in a low-dimensional space of two or three dimensions. Specifically, it models each high-dimensional object by a two- or three-dimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points with high probability, more information about this algorithm in its Wiki page . The tsne resource uses the t-SNE method on a dataset, and create a plot image from result.","title":"/api/learningOrchestra/v1/explore/tsne"},{"location":"t-sne-api/#create-an-image-plot","text":"POST CLUSTER_IP/api/learningOrchestra/v1/explore/tsne The body contains the json fields: { \"inputDatasetName\": \"used dataset name\", \"outputPlotName\": \"result image plot name\", \"label\": \"dataset label column\" } The label is the label name of the column for machine learning datasets which has labeled tuples. In the case that the dataset used doesn't contain labeled tuples, define the value as null type in json: { \"inputDatasetName\": \"used dataset name\", \"outputPlotName\": \"result image plot name\", \"label\": null }","title":"Create an image plot"},{"location":"t-sne-api/#read-the-names-of-the-created-plots","text":"GET CLUSTER_IP/api/learningOrchestra/v1/explore/tsne Returns a list with all created images plot names.","title":"Read the names of the created plots"},{"location":"t-sne-api/#read-an-image-plot","text":"GET CLUSTER_IP/api/learningOrchestra/v1/explore/tsne/<plotName> Returns the image plot of the specified plotName.","title":"Read an image plot"},{"location":"t-sne-api/#delete-an-image-plot","text":"DELETE CLUSTER_IP/api/learningOrchestra/v1/explore/tsne/<plotName> Deletes an image plot by specifying its plotName.","title":"Delete an image plot"},{"location":"t-sne-api/#image-plot-examples","text":"These examples use the titanic challengue datasets .","title":"Image plot examples"},{"location":"t-sne-api/#titanic-train-dataset","text":"","title":"Titanic Train dataset"},{"location":"t-sne-api/#titanic-test-dataset","text":"","title":"Titanic Test dataset"},{"location":"usage/","text":"Usage learningOrchestra can be used with the REST API or with the python package . REST API Dataset Dataset - Download and handle datasets in a database. Transform Projection - Make projections of stored datasets using Spark cluster. Data type - Change dataset fields type between number and text. Explore Histogram - Make histograms of stored datasets. t-SNE - Make a t-SNE image plot of stored datasets. PCA - Make a PCA image plot of stored datasets. Builder Builder - Create a prediction model from pre-processed datasets using Spark cluster. Spark Microservices The Projection, t-SNE, PCA and Builder uses the Spark microservice to work. By default, this microservice has only one instance. In case your data processing requires computing power, you need scale this microservice. To do this, with learningOrchestra already deployed, run the following in the manager machine of your Docker swarm cluster: docker service scale microservice_sparkworker=NUMBER_OF_INSTANCES * NUMBER_OF_INSTANCES is the number of Spark microservice instances which you require. Choose it according to your cluster resources and your resource requirements.","title":"Usage"},{"location":"usage/#usage","text":"learningOrchestra can be used with the REST API or with the python package .","title":"Usage"},{"location":"usage/#rest-api","text":"","title":"REST API"},{"location":"usage/#dataset","text":"Dataset - Download and handle datasets in a database.","title":"Dataset"},{"location":"usage/#transform","text":"Projection - Make projections of stored datasets using Spark cluster. Data type - Change dataset fields type between number and text.","title":"Transform"},{"location":"usage/#explore","text":"Histogram - Make histograms of stored datasets. t-SNE - Make a t-SNE image plot of stored datasets. PCA - Make a PCA image plot of stored datasets.","title":"Explore"},{"location":"usage/#builder","text":"Builder - Create a prediction model from pre-processed datasets using Spark cluster.","title":"Builder"},{"location":"usage/#spark-microservices","text":"The Projection, t-SNE, PCA and Builder uses the Spark microservice to work. By default, this microservice has only one instance. In case your data processing requires computing power, you need scale this microservice. To do this, with learningOrchestra already deployed, run the following in the manager machine of your Docker swarm cluster: docker service scale microservice_sparkworker=NUMBER_OF_INSTANCES * NUMBER_OF_INSTANCES is the number of Spark microservice instances which you require. Choose it according to your cluster resources and your resource requirements.","title":"Spark Microservices"}]}